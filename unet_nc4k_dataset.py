# -*- coding: utf-8 -*-
"""UNet_nc4k_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G3fTpgbdkmfhnNiaZi0rzTSidpBbuMzb
"""

import torch

print(torch.__version__)
print(torch.cuda.is_available())
if torch.cuda.is_available():
    print(torch.cuda.get_device_name(0))
else:
    print("CUDA not available. Running on CPU.")

from google.colab import drive
drive.mount('/content/drive')

!unzip "/content/drive/MyDrive/nc4k.zip" -d /content/nc4k

import os
print("Extracted folders:", os.listdir('/content/nc4k'))

dataset_path = '/content/nc4k'

def get_nc4k_folders(base_path):
    return {
        'images': f'{base_path}/Imgs',
        'gt_masks': f'{base_path}/GT',
        'instances': f'{base_path}/Instance'  # Optional for instance segmentation
    }

nc4k_folders = get_nc4k_folders(dataset_path)

def count_files(folder_dict):
    for key, folder in folder_dict.items():
        if os.path.exists(folder):
            print(f"{key}: {len(os.listdir(folder))} files")
        else:
            print(f"{key}: Folder not found")

print("NC4K Dataset:")
count_files(nc4k_folders)

import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import glob

class NC4KDataset(Dataset):
    def __init__(self, img_dir, mask_dir, transform=None, mask_transform=None):
        self.img_dir = img_dir
        self.mask_dir = mask_dir

        # Get all image files
        self.image_files = sorted(glob.glob(os.path.join(img_dir, '*')))
        self.transform = transform
        self.mask_transform = mask_transform

        # Filter images that have corresponding masks
        self.valid_pairs = []
        for img_path in self.image_files:
            img_name = os.path.basename(img_path)
            # Try different mask extensions
            mask_name = img_name.replace('.jpg', '.png').replace('.jpeg', '.png')
            mask_path = os.path.join(mask_dir, mask_name)

            if os.path.exists(mask_path):
                self.valid_pairs.append((img_path, mask_path))

        print(f"Found {len(self.valid_pairs)} valid image-mask pairs")

    def __len__(self):
        return len(self.valid_pairs)

    def __getitem__(self, idx):
        img_path, mask_path = self.valid_pairs[idx]

        # Load image and mask
        image = Image.open(img_path).convert('RGB')
        mask = Image.open(mask_path).convert('L')

        if self.transform:
            image = self.transform(image)
        if self.mask_transform:
            mask = self.mask_transform(mask)

        return image, mask

# Data transforms
transform = transforms.Compose([
    transforms.Resize((352, 352)),  # Common size for COD tasks
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

mask_transform = transforms.Compose([
    transforms.Resize((352, 352)),
    transforms.ToTensor()
])

# Create full dataset
full_dataset = NC4KDataset(
    img_dir=nc4k_folders['images'],
    mask_dir=nc4k_folders['gt_masks'],
    transform=transform,
    mask_transform=mask_transform
)

# Split dataset: 70% train, 20% val, 10% test
from torch.utils.data import random_split

total_size = len(full_dataset)
train_size = int(0.7 * total_size)
val_size = int(0.2 * total_size)
test_size = total_size - train_size - val_size

train_dataset, val_dataset, test_dataset = random_split(
    full_dataset,
    [train_size, val_size, test_size],
    generator=torch.Generator().manual_seed(42)  # For reproducibility
)

# Create dataloaders
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)

# Print dataset statistics
print(f"\nDataset Split:")
print(f"Total samples: {total_size}")
print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")
print(f"Test samples: {len(test_dataset)}")

# Test data loading
try:
    images, masks = next(iter(train_loader))
    print(f"\nBatch shapes:")
    print(f"Image batch shape: {images.shape}")
    print(f"Mask batch shape: {masks.shape}")
    print(f"Image value range: [{images.min():.3f}, {images.max():.3f}]")
    print(f"Mask value range: [{masks.min():.3f}, {masks.max():.3f}]")
except Exception as e:
    print(f"Error loading data: {e}")

import matplotlib.pyplot as plt
import torch

def visualize_samples_in_batches(dataset, num_samples=50, batch_size=10):
    num_batches = (num_samples + batch_size - 1) // batch_size
    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, num_samples)
        batch_size_actual = end_idx - start_idx

        fig, axes = plt.subplots(batch_size_actual, 3, figsize=(10,20))
        if batch_size_actual == 1:
            axes = [axes]  # If only one row, make axes iterable

        for i, idx in enumerate(range(start_idx, end_idx)):
            image, mask = dataset[idx]

            # Denormalize and clip image for visualization
            if image.max() <= 1:
                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
                image = image * std + mean
            image = torch.clamp(image, 0, 1)  # Always clip to [0, 1]

            # Original image
            axes[i, 0].imshow(image.permute(1, 2, 0).cpu().numpy())
            axes[i, 0].set_title('Image', fontsize=8)
            axes[i, 0].axis('off')

            # Ground truth mask (grayscale)
            axes[i, 1].imshow(mask.squeeze().cpu().numpy(), cmap='gray')
            axes[i, 1].set_title('Ground Truth Mask', fontsize=8)
            axes[i, 1].axis('off')

            # Heatmap of mask (jet colormap)
            axes[i, 2].imshow(mask.squeeze().cpu().numpy(), cmap='jet')
            axes[i, 2].set_title('Heatmap Mask', fontsize=8)
            axes[i, 2].axis('off')

        plt.tight_layout()
        plt.show()

# Usage:
visualize_samples_in_batches(full_dataset, num_samples=50, batch_size=10)

import torch
import torch.nn as nn
import torch.nn.functional as F

class UNet(nn.Module):
    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):
        super(UNet, self).__init__()
        self.downs = nn.ModuleList()
        self.ups = nn.ModuleList()

        # Down part (encoder)
        for feature in features:
            self.downs.append(DoubleConv(in_channels, feature))
            in_channels = feature

        # Up part (decoder)
        for feature in reversed(features):
            self.ups.append(
                nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2)
            )
            self.ups.append(DoubleConv(feature*2, feature))

        # Bottleneck
        self.bottleneck = DoubleConv(features[-1], features[-1]*2)

        # Final convolution
        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)

    def forward(self, x):
        skip_connections = []

        # Downsampling
        for down in self.downs:
            x = down(x)
            skip_connections.append(x)
            x = F.max_pool2d(x, kernel_size=2, stride=2)

        # Bottleneck
        x = self.bottleneck(x)

        # Upsampling
        for idx in range(0, len(self.ups), 2):
            x = self.ups[idx](x)
            skip_connection = skip_connections.pop()

            # Handle spatial dimension mismatches
            if x.shape != skip_connection.shape:
                x = F.interpolate(x, size=skip_connection.shape[2:], mode='bilinear', align_corners=True)

            concat_skip = torch.cat((skip_connection, x), dim=1)
            x = self.ups[idx+1](concat_skip)

        return self.final_conv(x)

class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(DoubleConv, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.conv(x)

# Example usage
if __name__ == "__main__":
    # Test with random input
    x = torch.randn((1, 3, 256, 256))
    model = UNet(in_channels=3, out_channels=1)
    preds = model(x)
    print(f"Input shape: {x.shape}")
    print(f"Output shape: {preds.shape}")  # Should be (1, 1, 256, 256)

from torchsummary import summary
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = UNet(in_channels=3, out_channels=1).to(device)

input_tensor = torch.randn((1, 3, 256, 256)).to(device)

summary(model, input_tensor.shape[1:])

import torch

def iou_score(preds, targets, threshold=0.5, eps=1e-6):
    preds = torch.sigmoid(preds)
    preds = (preds > threshold).float()
    targets = (targets > 0.5).float()
    intersection = (preds * targets).sum(dim=(1,2,3))
    union = preds.sum(dim=(1,2,3)) + targets.sum(dim=(1,2,3)) - intersection
    iou = (intersection + eps) / (union + eps)
    return iou.mean().item()

def dice_score(preds, targets, threshold=0.5, eps=1e-6):
    preds = torch.sigmoid(preds)
    preds = (preds > threshold).float()
    targets = (targets > 0.5).float()
    intersection = (preds * targets).sum(dim=(1,2,3))
    dice = (2 * intersection + eps) / (preds.sum(dim=(1,2,3)) + targets.sum(dim=(1,2,3)) + eps)
    return dice.mean().item()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import OneCycleLR
from torch.cuda.amp import GradScaler, autocast
from tqdm import tqdm

def train_unet(
    model,
    train_loader,
    val_loader,
    device,
    num_epochs=25,
    lr=1e-3,
    weight_decay=1e-4,
    max_lr=3e-3,
    grad_clip=1.0
):
    class HybridLoss(nn.Module):
        def __init__(self, alpha=0.5):
            super().__init__()
            self.alpha = alpha
            self.bce = nn.BCEWithLogitsLoss()

        def forward(self, pred, target):
            bce = self.bce(pred, target)
            pred_sig = torch.sigmoid(pred)
            intersection = (pred_sig * target).sum()
            dice = 1 - (2 * intersection + 1e-6) / (pred_sig.sum() + target.sum() + 1e-6)
            return self.alpha * bce + (1 - self.alpha) * dice

    model = model.to(device)
    criterion = HybridLoss(alpha=0.7)
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    scheduler = OneCycleLR(optimizer, max_lr=max_lr, steps_per_epoch=len(train_loader), epochs=num_epochs)
    scaler = GradScaler()

    loss_metrics = {
        'train_loss': [],
        'val_loss': []
    }

    for epoch in range(num_epochs):
        model.train()
        epoch_train_loss = 0.0

        for images, masks in tqdm(train_loader, desc=f"Train Epoch {epoch+1}/{num_epochs}"):
            images, masks = images.to(device), masks.to(device)

            with autocast():
                outputs = model(images)
                loss = criterion(outputs, masks)

            scaler.scale(loss).backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

            epoch_train_loss += loss.item() * images.size(0)

        model.eval()
        epoch_val_loss = 0.0

        with torch.no_grad():
            for images, masks in tqdm(val_loader, desc=f"Val Epoch {epoch+1}/{num_epochs}"):
                images, masks = images.to(device), masks.to(device)

                outputs = model(images)
                loss = criterion(outputs, masks)

                epoch_val_loss += loss.item() * images.size(0)

        loss_metrics['train_loss'].append(epoch_train_loss / len(train_loader.dataset))
        loss_metrics['val_loss'].append(epoch_val_loss / len(val_loader.dataset))

        scheduler.step()

        print(f"\nEpoch {epoch+1}/{num_epochs}")
        print(f"Train Loss: {loss_metrics['train_loss'][-1]:.4f} | Val Loss: {loss_metrics['val_loss'][-1]:.4f}")
        print("-----------------------------------")

    return loss_metrics

# Initialize model and dataloaders
model = UNet(in_channels=3, out_channels=1)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)

# Train with optimized settings
metrics = train_unet(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),
    num_epochs=25,
    lr=1e-3,
    max_lr=3e-3,
    grad_clip=1.0
)

import matplotlib.pyplot as plt

def plot_metrics(metrics, save_path=None):
    epochs = range(1, len(metrics['train_loss']) + 1)

    plt.figure(figsize=(20, 8))

    # Plot Loss
    plt.subplot(1, 3, 1)
    plt.plot(epochs, metrics['train_loss'], label='Train Loss')
    plt.plot(epochs, metrics['val_loss'], label='Val Loss')
    plt.title('Training & Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, bbox_inches='tight', dpi=300)
    plt.show()

plot_metrics(metrics, save_path='training_metrics.png')

import numpy as np
import matplotlib.pyplot as plt

def visualize_in_batches(model, loader, device, batch_size=32, images_per_row=8):
    model.eval()
    # ImageNet mean and std for denormalization
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    for batch_idx, (images, _) in enumerate(loader):
        with torch.no_grad():
            images = images.to(device)
            outputs = model(images)
            probs = torch.sigmoid(outputs).cpu().numpy()

        num_images = images.shape[0]
        rows = int(np.ceil(num_images / images_per_row))

        plt.figure(figsize=(2.5*images_per_row, 2.5*rows))
        for idx in range(num_images):
            img = images[idx].cpu().permute(1, 2, 0).numpy()
            # Denormalize and clip
            img = img * std + mean
            img = np.clip(img, 0, 1)

            prob_map = probs[idx].squeeze()

            plt.subplot(rows, images_per_row*2, 2*idx+1)
            plt.imshow(img)
            plt.axis('off')

            plt.subplot(rows, images_per_row*2, 2*idx+2)
            plt.imshow(img)
            plt.imshow(prob_map, cmap='jet', alpha=0.5)
            plt.axis('off')

        plt.tight_layout()
        plt.suptitle(f'Batch {batch_idx+1}', y=1.02)
        plt.show()
        plt.close()
visualize_in_batches(model, test_loader, device, batch_size=32, images_per_row=8)